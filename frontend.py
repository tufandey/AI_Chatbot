# ğŸ¯ Load environment variables
from dotenv import load_dotenv
load_dotenv()

# ğŸ“¦ Import libraries
import os
import streamlit as st
from langchain_groq import ChatGroq
from langgraph.prebuilt import create_react_agent
from langchain_core.messages.ai import AIMessage
from langchain_community.tools.tavily_search import TavilySearchResults

# ğŸ” Read API keys
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")

# ğŸ¨ Streamlit page configuration
st.set_page_config(page_title="ğŸ¤– TufanBot", layout="centered")

# ğŸ–¼ï¸ Optional: Add a banner or avatar image
st.image("https://cdn-icons-png.flaticon.com/512/4712/4712035.png", width=80)
st.markdown("<h1 style='text-align: center;'>ğŸ¤– TufanBot</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align: center;'>Your friendly AI assistant powered by Groq âœ¨</p>", unsafe_allow_html=True)
st.markdown("---")

# ğŸ§  Agent Personality
system_prompt = st.text_area(
    "ğŸ§  Define your AI's Role (e.g., Doctor, Developer, Travel Buddy):",
    height=70,
    placeholder="Type the role or personality of your AI agent..."
)

# ğŸŒ Enable optional web search
allow_web_search = st.checkbox("ğŸŒ Enable Web Search via Tavily")

# âœï¸ User Query Input
user_query = st.text_area("ğŸ’¬ Ask Anything:", height=150, placeholder="Type your question here...")

# ğŸ§  Fixed model to Groq's llama-3.3-70b
selected_model = "llama-3.3-70b-versatile"

# ğŸš€ Trigger response
if st.button("ğŸš€ Ask Tufan!"):
    if user_query.strip():
        try:
            status = st.empty()
            status.info("ğŸ¤” TufanBot is thinking...")

            # ğŸ’¡ Initialize the Groq LLM
            llm = ChatGroq(model=selected_model)

            # ğŸ”§ Use search tool if enabled
            tools = [TavilySearchResults(max_results=2)] if allow_web_search else []

            # ğŸ¤– Create reactive agent
            agent = create_react_agent(
                model=llm,
                tools=tools,
                state_modifier=system_prompt
            )

            # ğŸ“¨ Query the agent
            state = {"messages": user_query}
            result = agent.invoke(state)
            messages = result.get("messages", [])
            ai_messages = [msg.content for msg in messages if isinstance(msg, AIMessage)]

            # âœ… Hide thinking message
            status.empty()

            # ğŸ“¬ Show AI response
            if ai_messages:
                st.subheader("ğŸ§¾ TufanBot says:")
                st.markdown(f"**{ai_messages[-1]}**")
            else:
                st.warning("âš ï¸ No response received.")

        except Exception as e:
            status.empty()
            st.error(f"ğŸ’¥ Error: {e}")
